{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "McraDkpdn0cL"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from IPython.display import Image  # for displaying images\n",
        "import os \n",
        "import random\n",
        "import shutil\n",
        "from sklearn.model_selection import train_test_split\n",
        "import xml.etree.ElementTree as ET\n",
        "from xml.dom import minidom\n",
        "from tqdm import tqdm\n",
        "from PIL import Image, ImageDraw\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "random.seed(108)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lj3Lt1B27B78"
      },
      "source": [
        "Training images set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ddkclTICuhwY"
      },
      "outputs": [],
      "source": [
        "!wget https://sid.erda.dk/public/archives/daaeac0d7ce1152aea9b61d9f1e19370/GTSRB-Training_fixed.zip\n",
        "!unzip /content/GTSRB-Training_fixed.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "niwMcNT-6DwE"
      },
      "outputs": [],
      "source": [
        "! pip install kaggle\n",
        "! mkdir ~/.kaggle\n",
        "! cp /content/drive/MyDrive/major_project/kaggle/kaggle.json ~/.kaggle/\n",
        "! chmod 600 ~/.kaggle/kaggle.json\n",
        "! kaggle competitions download <name-of-competition>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D70PYPMq7B1-"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c3fNt2dShPSG"
      },
      "outputs": [],
      "source": [
        "!curl -L \"https://public.roboflow.com/ds/ONxyYx8ddR?key=36n4EfSQY5\" > roboflow.zip; unzip roboflow.zip; rm roboflow.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "34rBkJtyDZs-",
        "outputId": "e8ee0aa6-cf79-4ff9-ef4c-5781a9c6befd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C0MUFqI5FAtZ"
      },
      "outputs": [],
      "source": [
        "def cleanFolder(folderName):\n",
        "  annotations = [os.path.join(folderName, x) for x in os.listdir(folderName)]\n",
        "  for ann in tqdm(annotations):\n",
        "    os.remove(ann)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IKUEnS7IP3FZ",
        "outputId": "3f51e2a0-a67e-48f5-803a-9225efb41c15"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 74/74 [00:00<00:00, 22750.02it/s]\n"
          ]
        }
      ],
      "source": [
        "cleanFolder(\"/content/GTSRBedited/images\");"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0aYq1fwXFLny"
      },
      "outputs": [],
      "source": [
        "!mkdir GTSRBedited\n",
        "!mkdir GTSRBedited/images\n",
        "!mkdir GTSRBedited/labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EwWmfvS8E0RS"
      },
      "outputs": [],
      "source": [
        "def saveLabels(csvPath, classId, fileNamePrefix):\n",
        "  data  = pd.read_csv(csvPath, sep = ';')\n",
        "\n",
        "  for i in range(len(data[\"Width\"])):\n",
        "    imgWidth = data[\"Width\"][i]\n",
        "    imgHeight = data[\"Height\"][i]\n",
        "    b_center_x = ((data[\"Roi.X1\"][i] + data[\"Roi.X2\"][i]) / 2)/imgWidth \n",
        "    b_center_y = ((data[\"Roi.Y1\"][i] + data[\"Roi.Y2\"][i]) / 2)/imgHeight\n",
        "    b_width    = ((data[\"Roi.X2\"][i] - data[\"Roi.X1\"][i]))/imgWidth\n",
        "    b_height   = ((data[\"Roi.Y2\"][i] - data[\"Roi.Y1\"][i]))/imgHeight\n",
        "\n",
        "    correspondingImageName = data[\"Filename\"][i];\n",
        "\n",
        "    destFolder = \"/content/GTSRBedited/labels/\"\n",
        "\n",
        "    finalPath  = destFolder + fileNamePrefix + correspondingImageName.replace('ppm', 'txt')\n",
        "\n",
        "    f = open(finalPath, \"w\")\n",
        "    f.write(str(classId)+\" \"+str(b_center_x)+\" \"+str(b_center_y)+\" \"+str(b_width)+\" \"+str(b_height))\n",
        "    \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YFnGDEM2JtxJ"
      },
      "outputs": [],
      "source": [
        "csvPath = [\"/content/GTSRB/Training/00000/GT-00000.csv\", \"/content/GTSRB/Training/00007/GT-00007.csv\", \"/content/GTSRB/Training/00014/GT-00014.csv\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0bA5BquNJ9Gq"
      },
      "outputs": [],
      "source": [
        "saveLabels(csvPath[0], 11, \"00000_\")\n",
        "saveLabels(csvPath[1], 12, \"00007_\")\n",
        "saveLabels(csvPath[2], 13, \"00014_\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i9BSV4VFLOem"
      },
      "outputs": [],
      "source": [
        "def convertPpmToJpgAndSave(imagesFolder, prefix):\n",
        "  imagesPath = [os.path.join(imagesFolder, x) for x in os.listdir(imagesFolder) if x[-3:] == \"ppm\"]\n",
        "  imagesName = os.listdir(imagesFolder)\n",
        "  \n",
        "  #print(imagesName[0], imagesPath[0], imagesName[3], imagesPath[3])\n",
        "  \n",
        "  imagesPath.sort()\n",
        "  imagesName.sort()\n",
        "\n",
        "  #sort is necessary above, to find out check the loop below\n",
        "  #or remove above two lines and exectue\n",
        "\n",
        "  for i in range(len(imagesPath)):\n",
        "    \n",
        "    imageInPpm = Image.open(imagesPath[i])\n",
        "    if \"csv\" in imagesName[i]:\n",
        "      print(imagesName[i], \" at index \", i, \" length is \", len(imagesPath))\n",
        "      break;\n",
        "    destFolder = \"/content/GTSRBedited/images/\"\n",
        "    PathOfImageInJpg = destFolder + prefix + imagesName[i].replace(\"ppm\", \"jpg\")\n",
        "    imageInPpm.save(PathOfImageInJpg)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7k_KzMYUPAxe"
      },
      "outputs": [],
      "source": [
        "imagesPath = [\"/content/GTSRB/Training/00000/\", \"/content/GTSRB/Training/00007/\", \"/content/GTSRB/Training/00014/\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MIImreGWM08w"
      },
      "outputs": [],
      "source": [
        "convertPpmToJpgAndSave(imagesPath[0], \"00000_\")\n",
        "convertPpmToJpgAndSave(imagesPath[1], \"00007_\")\n",
        "convertPpmToJpgAndSave(imagesPath[2], \"000014_\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MalSn262YIkR"
      },
      "outputs": [],
      "source": [
        "def moveFilesFrom(sourceFolder, toDestFolder):\n",
        "  imagesInSourceFolder = os.listdir(sourceFolder);\n",
        "  for imageName in tqdm(imagesInSourceFolder):\n",
        "    sourceFilePath = sourceFolder + imageName\n",
        "    destFilePath = toDestFolder + imageName\n",
        "    shutil.move(sourceFilePath, destFilePath)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hdQq8iB9YIf4",
        "outputId": "4d83d7ab-267a-4fea-ff87-b3304c1115b3"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1650/1650 [00:00<00:00, 34926.78it/s]\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1650/1650 [00:00<00:00, 33085.86it/s]\n"
          ]
        }
      ],
      "source": [
        "sourceFolder = [\"/content/GTSRBedited/images/\", \"/content/GTSRBedited/labels/\"]\n",
        "toDestFolder = [\"/content/export/images/\", \"/content/export/labels/\"]\n",
        "for i in range(len(sourceFolder)):\n",
        "  moveFilesFrom(sourceFolder[i], toDestFolder[i])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oc2LjOtwi_ha"
      },
      "outputs": [],
      "source": [
        "import xml.etree.ElementTree as ET"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oYQ8N373ipU5"
      },
      "outputs": [],
      "source": [
        "# Function to get the data from XML Annotation\n",
        "def extract_info_from_xml(xml_file):\n",
        "    root = ET.parse(xml_file).getroot()\n",
        "    \n",
        "    # Initialise the info dict \n",
        "    info_dict = {}\n",
        "    info_dict['bboxes'] = []\n",
        "\n",
        "    # Parse the XML Tree\n",
        "    for elem in root:\n",
        "        # Get the file name \n",
        "        if elem.tag == \"filename\":\n",
        "            info_dict['filename'] = elem.text\n",
        "            \n",
        "        # Get the image size\n",
        "        elif elem.tag == \"size\":\n",
        "            image_size = []\n",
        "            for subelem in elem:\n",
        "                image_size.append(int(subelem.text))\n",
        "            \n",
        "            info_dict['image_size'] = tuple(image_size)\n",
        "        \n",
        "        # Get details of the bounding box \n",
        "        elif elem.tag == \"object\":\n",
        "            bbox = {}\n",
        "            for subelem in elem:\n",
        "                if subelem.tag == \"name\":\n",
        "                    bbox[\"class\"] = subelem.text\n",
        "                    \n",
        "                elif subelem.tag == \"bndbox\":\n",
        "                    for subsubelem in subelem:\n",
        "                        bbox[subsubelem.tag] = int(subsubelem.text)            \n",
        "            info_dict['bboxes'].append(bbox)\n",
        "    \n",
        "    return info_dict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mNDSoIhuipM9"
      },
      "outputs": [],
      "source": [
        "print(extract_info_from_xml('annotations/road4.xml'))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EZgH-zUukvSO"
      },
      "outputs": [],
      "source": [
        "# Dictionary that maps class names to IDs\n",
        "class_name_to_id_mapping = {\"trafficlight\": 0,\n",
        "                           \"stop\": 1,\n",
        "                           \"speedlimit\": 2,\n",
        "                           \"crosswalk\": 3}\n",
        "\n",
        "# Convert the info dict to the required yolo format and write it to disk\n",
        "def convert_to_yolov5(info_dict):\n",
        "    print_buffer = []\n",
        "    \n",
        "    # For each bounding box\n",
        "    for b in info_dict[\"bboxes\"]:\n",
        "        try:\n",
        "            class_id = class_name_to_id_mapping[b[\"class\"]]\n",
        "        except KeyError:\n",
        "            print(\"Invalid Class. Must be one from \", class_name_to_id_mapping.keys())\n",
        "        \n",
        "        # Transform the bbox co-ordinates as per the format required by YOLO v5\n",
        "        b_center_x = (b[\"xmin\"] + b[\"xmax\"]) / 2 \n",
        "        b_center_y = (b[\"ymin\"] + b[\"ymax\"]) / 2\n",
        "        b_width    = (b[\"xmax\"] - b[\"xmin\"])\n",
        "        b_height   = (b[\"ymax\"] - b[\"ymin\"])\n",
        "        \n",
        "        # Normalise the co-ordinates by the dimensions of the image\n",
        "        image_w, image_h, image_c = info_dict[\"image_size\"]  \n",
        "        b_center_x /= image_w \n",
        "        b_center_y /= image_h \n",
        "        b_width    /= image_w \n",
        "        b_height   /= image_h \n",
        "        \n",
        "        #Write the bbox details to the file \n",
        "        print_buffer.append(\"{} {:.3f} {:.3f} {:.3f} {:.3f}\".format(class_id, b_center_x, b_center_y, b_width, b_height))\n",
        "        \n",
        "    # Name of the file which we have to save \n",
        "    save_file_name = os.path.join(\"annotations\", info_dict[\"filename\"].replace(\"png\", \"txt\"))\n",
        "    \n",
        "    # Save the annotation to disk\n",
        "    print(\"\\n\".join(print_buffer), file= open(save_file_name, \"w\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vR6ZaTVSnBOo"
      },
      "outputs": [],
      "source": [
        "# Read images and annotations\n",
        "\n",
        "trainImagesPath = \"/content/export/images/\"\n",
        "labelsPath = \"/content/export/labels/\"\n",
        "\n",
        "images = [os.path.join(trainImagesPath, x) for x in os.listdir(trainImagesPath)]\n",
        "annotations = [os.path.join(labelsPath, x) for x in os.listdir(labelsPath)]\n",
        "\n",
        "images.sort()\n",
        "annotations.sort()\n",
        "\n",
        "# Split the dataset into train-valid-test splits \n",
        "train_images, val_images, train_annotations, val_annotations = train_test_split(images, annotations, test_size = 0.2, random_state = 1)\n",
        "#val_images, test_images, val_annotations, test_annotations = train_test_split(val_images, val_annotations, test_size = 0.5, random_state = 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "23MZXesTdeIN"
      },
      "outputs": [],
      "source": [
        "!mkdir train\n",
        "!mkdir train/images train/labels\n",
        "!mkdir train/images/train train/images/val train/labels/train train/labels/val"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gGgWnJCBoeV8"
      },
      "outputs": [],
      "source": [
        "#Utility function to move images \n",
        "def move_files_to_folder(list_of_files, destination_folder):\n",
        "    for f in list_of_files:\n",
        "        try:\n",
        "            shutil.move(f, destination_folder)\n",
        "        except:\n",
        "            print(f)\n",
        "            assert False\n",
        "\n",
        "# Move the splits into their folders\n",
        "move_files_to_folder(train_images, 'train/images/train')\n",
        "move_files_to_folder(val_images, 'train/images/val/')\n",
        "#move_files_to_folder(test_images, 'images/test/')\n",
        "move_files_to_folder(train_annotations, 'train/labels/train/')\n",
        "move_files_to_folder(val_annotations, 'train/labels/val/')\n",
        "#move_files_to_folder(test_annotations, 'annotations/test/')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wp53g-TIpyoX",
        "outputId": "7f8a76d6-982e-4274-8c8b-2747050b15cf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'yolov5'...\n",
            "remote: Enumerating objects: 14992, done.\u001b[K\n",
            "remote: Counting objects: 100% (3/3), done.\u001b[K\n",
            "remote: Compressing objects: 100% (3/3), done.\u001b[K\n",
            "remote: Total 14992 (delta 0), reused 1 (delta 0), pack-reused 14989\u001b[K\n",
            "Receiving objects: 100% (14992/14992), 14.01 MiB | 16.50 MiB/s, done.\n",
            "Resolving deltas: 100% (10294/10294), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/ultralytics/yolov5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "RX3w97acthAY",
        "outputId": "7d7bfc6a-8f74-40a5-d97b-d9af2384aa69"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mtrain: \u001b[0mweights=yolov5s.pt, cfg=/content/yolov5/models/yolov5n.yaml, data=/content/yolov5/data/data.yaml, hyp=/content/yolov5/data/hyps/hyp.scratch-low.yaml, epochs=25, batch_size=128, imgsz=480, rect=False, resume=False, nosave=False, noval=False, noautoanchor=False, noplots=False, evolve=None, bucket=, cache=ram, image_weights=False, device=, multi_scale=False, single_cls=False, optimizer=SGD, sync_bn=False, workers=24, project=yolov5/runs/train, name=udacity_gtsrb, exist_ok=False, quad=False, cos_lr=False, label_smoothing=0.0, patience=100, freeze=[0], save_period=-1, seed=0, local_rank=-1, entity=None, upload_dataset=False, bbox_interval=-1, artifact_alias=latest\n",
            "\u001b[34m\u001b[1mgithub: \u001b[0mup to date with https://github.com/ultralytics/yolov5 âœ…\n",
            "YOLOv5 ðŸš€ v7.0-70-g589edc7 Python-3.8.16 torch-1.13.0+cu116 CUDA:0 (Tesla T4, 15110MiB)\n",
            "\n",
            "\u001b[34m\u001b[1mhyperparameters: \u001b[0mlr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=0.05, cls=0.5, cls_pw=1.0, obj=1.0, obj_pw=1.0, iou_t=0.2, anchor_t=4.0, fl_gamma=0.0, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0\n",
            "\u001b[34m\u001b[1mClearML: \u001b[0mrun 'pip install clearml' to automatically track, visualize and remotely train YOLOv5 ðŸš€ in ClearML\n",
            "\u001b[34m\u001b[1mComet: \u001b[0mrun 'pip install comet_ml' to automatically track and visualize YOLOv5 ðŸš€ runs in Comet\n",
            "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir yolov5/runs/train', view at http://localhost:6006/\n",
            "Downloading https://ultralytics.com/assets/Arial.ttf to /root/.config/Ultralytics/Arial.ttf...\n",
            "100% 755k/755k [00:00<00:00, 23.4MB/s]\n",
            "Downloading https://github.com/ultralytics/yolov5/releases/download/v7.0/yolov5s.pt to yolov5s.pt...\n",
            "100% 14.1M/14.1M [00:00<00:00, 203MB/s]\n",
            "\n",
            "Overriding model.yaml nc=80 with nc=14\n",
            "\n",
            "                 from  n    params  module                                  arguments                     \n",
            "  0                -1  1      1760  models.common.Conv                      [3, 16, 6, 2, 2]              \n",
            "  1                -1  1      4672  models.common.Conv                      [16, 32, 3, 2]                \n",
            "  2                -1  1      4800  models.common.C3                        [32, 32, 1]                   \n",
            "  3                -1  1     18560  models.common.Conv                      [32, 64, 3, 2]                \n",
            "  4                -1  2     29184  models.common.C3                        [64, 64, 2]                   \n",
            "  5                -1  1     73984  models.common.Conv                      [64, 128, 3, 2]               \n",
            "  6                -1  3    156928  models.common.C3                        [128, 128, 3]                 \n",
            "  7                -1  1    295424  models.common.Conv                      [128, 256, 3, 2]              \n",
            "  8                -1  1    296448  models.common.C3                        [256, 256, 1]                 \n",
            "  9                -1  1    164608  models.common.SPPF                      [256, 256, 5]                 \n",
            " 10                -1  1     33024  models.common.Conv                      [256, 128, 1, 1]              \n",
            " 11                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
            " 12           [-1, 6]  1         0  models.common.Concat                    [1]                           \n",
            " 13                -1  1     90880  models.common.C3                        [256, 128, 1, False]          \n",
            " 14                -1  1      8320  models.common.Conv                      [128, 64, 1, 1]               \n",
            " 15                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
            " 16           [-1, 4]  1         0  models.common.Concat                    [1]                           \n",
            " 17                -1  1     22912  models.common.C3                        [128, 64, 1, False]           \n",
            " 18                -1  1     36992  models.common.Conv                      [64, 64, 3, 2]                \n",
            " 19          [-1, 14]  1         0  models.common.Concat                    [1]                           \n",
            " 20                -1  1     74496  models.common.C3                        [128, 128, 1, False]          \n",
            " 21                -1  1    147712  models.common.Conv                      [128, 128, 3, 2]              \n",
            " 22          [-1, 10]  1         0  models.common.Concat                    [1]                           \n",
            " 23                -1  1    296448  models.common.C3                        [256, 256, 1, False]          \n",
            " 24      [17, 20, 23]  1     25707  models.yolo.Detect                      [14, [[10, 13, 16, 30, 33, 23], [30, 61, 62, 45, 59, 119], [116, 90, 156, 198, 373, 326]], [64, 128, 256]]\n",
            "YOLOv5n summary: 214 layers, 1782859 parameters, 1782859 gradients, 4.3 GFLOPs\n",
            "\n",
            "Transferred 57/349 items from yolov5s.pt\n",
            "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed âœ…\n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m SGD(lr=0.01) with parameter groups 57 weight(decay=0.0), 60 weight(decay=0.001), 60 bias\n",
            "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01), CLAHE(p=0.01, clip_limit=(1, 4.0), tile_grid_size=(8, 8))\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /content/train/labels/train... 30612 images, 4338 backgrounds, 0 corrupt: 100% 31450/31450 [00:18<00:00, 1690.24it/s]\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING âš ï¸ /content/train/images/train/1478021875081281646_jpg.rf.bEZPhuyXU5hIovwQSTIp.jpg: 1 duplicate labels removed\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING âš ï¸ /content/train/images/train/1478021875081281646_jpg.rf.e9552980cf8c6fef4aa02cb84c6364f5.jpg: 1 duplicate labels removed\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING âš ï¸ /content/train/images/train/1478897760163798179_jpg.rf.5Pzrj3Eg3vZuyl7ztKAt.jpg: 1 duplicate labels removed\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING âš ï¸ /content/train/images/train/1478898145212453716_jpg.rf.6a92d7d7dd523160c990c4e4375bcea9.jpg: 1 duplicate labels removed\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING âš ï¸ /content/train/images/train/1478898145212453716_jpg.rf.nCaFkPk4AFMjTQAM4RTJ.jpg: 1 duplicate labels removed\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING âš ï¸ /content/train/images/val/1478897760163798179_jpg.rf.98623be50b02ff17d58f89fddf7a0c6c.jpg: 1 duplicate labels removed\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: /content/train/labels/train.cache\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0m20.2GB RAM required, 9.2/12.7GB available, not caching images âš ï¸\n",
            "\u001b[34m\u001b[1mval: \u001b[0mScanning /content/train/labels/val... 6030 images, 989 backgrounds, 0 corrupt: 100% 6290/6290 [00:09<00:00, 676.68it/s]\n",
            "\u001b[34m\u001b[1mval: \u001b[0mWARNING âš ï¸ /content/train/images/val/1478897760163798179_jpg.rf.98623be50b02ff17d58f89fddf7a0c6c.jpg: 1 duplicate labels removed\n",
            "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: /content/train/labels/val.cache\n",
            "\u001b[34m\u001b[1mval: \u001b[0mCaching images (4.0GB ram): 100% 6290/6290 [00:57<00:00, 109.04it/s]\n",
            "\n",
            "\u001b[34m\u001b[1mAutoAnchor: \u001b[0m4.87 anchors/target, 0.999 Best Possible Recall (BPR). Current anchors are a good fit to dataset âœ…\n",
            "Plotting labels to yolov5/runs/train/udacity_gtsrb2/labels.jpg... \n",
            "Image sizes 480 train, 480 val\n",
            "Using 2 dataloader workers\n",
            "Logging results to \u001b[1myolov5/runs/train/udacity_gtsrb2\u001b[0m\n",
            "Starting training for 25 epochs...\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "       0/24      8.34G    0.09986    0.04593    0.03878        806        480: 100% 246/246 [13:00<00:00,  3.17s/it]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 25/25 [01:09<00:00,  2.79s/it]\n",
            "                   all       6290      39268      0.937     0.0159    0.00768    0.00194\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "       1/24      9.97G    0.08047    0.03812    0.02396        778        480: 100% 246/246 [13:30<00:00,  3.30s/it]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 25/25 [00:53<00:00,  2.12s/it]\n",
            "                   all       6290      39268      0.967     0.0332     0.0392     0.0132\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "       2/24      9.97G    0.07085    0.03605    0.01942        703        480: 100% 246/246 [13:27<00:00,  3.28s/it]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 25/25 [00:48<00:00,  1.92s/it]\n",
            "                   all       6290      39268      0.402      0.173     0.0878     0.0331\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "       3/24      9.97G    0.06267    0.03387    0.01522        826        480: 100% 246/246 [13:29<00:00,  3.29s/it]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 25/25 [00:50<00:00,  2.02s/it]\n",
            "                   all       6290      39268      0.511      0.195      0.139     0.0547\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "       4/24      9.97G    0.05785     0.0324    0.01316        733        480: 100% 246/246 [13:34<00:00,  3.31s/it]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 25/25 [00:47<00:00,  1.89s/it]\n",
            "                   all       6290      39268      0.661      0.236      0.193     0.0832\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "       5/24      9.97G    0.05481     0.0312    0.01198        775        480: 100% 246/246 [13:32<00:00,  3.30s/it]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 25/25 [00:46<00:00,  1.87s/it]\n",
            "                   all       6290      39268      0.478      0.298      0.246      0.113\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "       6/24      9.97G    0.05275    0.03056     0.0112        787        480: 100% 246/246 [13:26<00:00,  3.28s/it]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 25/25 [00:47<00:00,  1.91s/it]\n",
            "                   all       6290      39268      0.486      0.379      0.279      0.128\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "       7/24      9.97G    0.05126     0.0302    0.01061        760        480: 100% 246/246 [13:31<00:00,  3.30s/it]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 25/25 [00:46<00:00,  1.86s/it]\n",
            "                   all       6290      39268      0.524      0.377      0.299      0.145\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "       8/24      9.97G    0.05001    0.02959    0.01003        918        480: 100% 246/246 [13:23<00:00,  3.26s/it]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 25/25 [00:48<00:00,  1.92s/it]\n",
            "                   all       6290      39268      0.655      0.386       0.34      0.168\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "       9/24      9.97G    0.04902    0.02935   0.009616        804        480: 100% 246/246 [12:56<00:00,  3.16s/it]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 25/25 [00:44<00:00,  1.77s/it]\n",
            "                   all       6290      39268      0.628      0.376      0.368      0.184\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      10/24      9.97G     0.0483    0.02914    0.00922       1167        480:  41% 102/246 [05:20<08:29,  3.54s/it]"
          ]
        }
      ],
      "source": [
        "!python /content/yolov5/train.py --img 480 --cfg /content/yolov5/models/yolov5n.yaml --hyp /content/yolov5/data/hyps/hyp.scratch-low.yaml --batch 128 --epochs 25 --cache ram --weights yolov5s.pt --data /content/yolov5/data/data.yaml  --workers 24 --name udacity_gtsrb"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZUcvjPwoCpNz"
      },
      "source": [
        "CHANGE the \"source\" and \"weights\" attribute below\n",
        "Take weights as best.pt from path mentioned by yolo trainer after training is finished ---> \"Results saved to yolov5/runs ..... \""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vl93ymL34i3R",
        "outputId": "4d46e9f4-3d8d-44cc-ffdb-516894e31653"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mdetect: \u001b[0mweights=['/content/drive/MyDrive/major_project/best.pt'], source=https://www.gannett-cdn.com/presto/2018/09/09/PVCS/4e811a87-1147-4195-b237-90d27238a8fa-20180909_220853000_iOS.jpg, data=yolov5/data/coco128.yaml, imgsz=[640, 640], conf_thres=0.25, iou_thres=0.45, max_det=1000, device=, view_img=False, save_txt=False, save_conf=False, save_crop=False, nosave=False, classes=None, agnostic_nms=False, augment=False, visualize=False, update=False, project=yolov5/runs/detect, name=exp, exist_ok=False, line_thickness=3, hide_labels=False, hide_conf=False, half=False, dnn=False, vid_stride=1\n",
            "Found https://www.gannett-cdn.com/presto/2018/09/09/PVCS/4e811a87-1147-4195-b237-90d27238a8fa-20180909_220853000_iOS.jpg locally at 4e811a87-1147-4195-b237-90d27238a8fa-20180909_220853000_iOS.jpg\n",
            "YOLOv5 ðŸš€ v7.0-69-g3b6e27a Python-3.8.16 torch-1.13.0+cu116 CUDA:0 (Tesla T4, 15110MiB)\n",
            "\n",
            "Fusing layers... \n",
            "YOLOv5s summary: 213 layers, 7020913 parameters, 0 gradients\n",
            "image 1/1 /content/4e811a87-1147-4195-b237-90d27238a8fa-20180909_220853000_iOS.jpg: 480x640 1 stop, 12.4ms\n",
            "Speed: 0.5ms pre-process, 12.4ms inference, 1.5ms NMS per image at shape (1, 3, 640, 640)\n",
            "Results saved to \u001b[1myolov5/runs/detect/exp2\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!python yolov5/detect.py --source https://www.gannett-cdn.com/presto/2018/09/09/PVCS/4e811a87-1147-4195-b237-90d27238a8fa-20180909_220853000_iOS.jpg  --weights /content/drive/MyDrive/major_project/best.pt\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mxN_84zXDm-p"
      },
      "outputs": [],
      "source": [
        "print(result.length)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f0Mh1TQbAkWZ",
        "outputId": "b3af7dd8-fcd6-460a-c591-e4de6c61b739"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0 ... 0 ... 0 ...\n"
          ]
        }
      ],
      "source": [
        "isStop = \"stop car\" if (\"stop\" in result) else 0\n",
        "isSpeedLimit = \"slow down the car\" if (\"speedlimit\" in result) else 0\n",
        "isTrafficLight = \"traffic light detected\" if(\"traffic\" in result) else 0\n",
        "print(isStop, \"...\", isSpeedLimit, \"...\", isTrafficLight, \"...\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}